torch
numpy
transformers==4.26.1
tokenizers==0.13.2
onnx
openvino==2022.3.0
openvino-dev[onnx,pytorch]==2022.3.0      # For model optimizer
numba   # Fast softmax per: https://www.bragitoff.com/2021/12/efficient-implementation-of-softmax-activation-function-and-its-derivative-jacobian-in-python/
neural-compressor
datasets
evaluate
optimum[neural-compressor]